{"cells":[{"cell_type":"markdown","metadata":{"id":"yPXhYbxILFek"},"source":["# Install libraries"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19941,"status":"ok","timestamp":1696005631425,"user":{"displayName":"Dojun Park","userId":"14496627616213037898"},"user_tz":-540},"id":"DP1IM5KvEhnM","outputId":"b59db13b-99f0-4602-db7c-6191d74ad75f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3479,"status":"ok","timestamp":1696005786979,"user":{"displayName":"Dojun Park","userId":"14496627616213037898"},"user_tz":-540},"id":"8EwjhVSGrLCC","outputId":"9e38ed29-c759-44f7-a99e-c53840ce1ed7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.33.3)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.17.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n","Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.15.1->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6086,"status":"ok","timestamp":1696005793061,"user":{"displayName":"Dojun Park","userId":"14496627616213037898"},"user_tz":-540},"id":"asLnhJUQ3WmI","outputId":"359cd9c8-0964-4131-f2e5-7d3d9d8f2471"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: SentencePiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}],"source":["!pip install SentencePiece"]},{"cell_type":"markdown","metadata":{"id":"WQP1RLJNBgry"},"source":["# Data"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":380},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696005793061,"user":{"displayName":"Dojun Park","userId":"14496627616213037898"},"user_tz":-540},"id":"FyDiDRlHniGq","outputId":"a670b4ad-8b89-4566-80cc-27588d73b05c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["       type        corpus                                          en_source  \\\n","0  Training  GlobalVoices  To let Japanese buy FT will be the last thing ...   \n","1  Training  GlobalVoices  If our uncritical engagement with media is any...   \n","2  Training  GlobalVoices  China: Rising prices and rooftop gardens · Glo...   \n","3  Training  GlobalVoices  And one year of a brave stance against great e...   \n","4  Training  GlobalVoices  Amina's story deeply touched and outraged Moro...   \n","\n","                                        ko_reference  \\\n","0              중국은 닛케이가 파이낸셜타임스를 인수하는 걸 절대 바라지 않는다.    \n","1  미디어와의 이런 안식없는 관계는 우리가 스스로와도 밀접하게 연관되어 있지 못하는 것...   \n","2                         중국: 물가상승에 따른 새로운 현상- 옥상텃밭    \n","3  그리고 일 년 간 거대한 악에 대항하는 용기도 볼 수 있었다. 그러나 시리아인들은 ...   \n","4  모로코 누리꾼들은 아미나의 이야기를 듣고 분노했고, 트위터 해쉬태그 #RIPAmin...   \n","\n","                                           ko_target  \\\n","0            중국은 일본 고객의 FT 구매를 최종 선호로 보고 싶지 않을 것입니다.   \n","1      우리가 언론을 세심하게 살피지 않는 걸 보면 우리 일에는 관심조차 없는 것 같다.   \n","2  글로벌 보이스(Global Voices)는 중국의 치솟는 물가와 옥상 정원의 등장에...   \n","3  1년 내내 강력한 세력에 맞서는 용기를 보여줬음에도 불구하고, 시리아 시민들은 자유...   \n","4  Amina의 이야기는 Twitter에서 해시태그 #RIPAmina를 사용하여 소녀를...   \n","\n","                                          annotation  accuracy  fluency  \\\n","0  Accuracy: 일본 고객의(mistranslation/major), 최종 선호로...      35.0      5.0   \n","1  Accuracy: 우리가 언론을 세심하게 살피지 않는 걸 보면(mistranslat...       7.0      0.0   \n","2  Accuracy: 등장에(addition/minor), (Global Voices)...       3.0      2.0   \n","3  Accuracy: 세력에(mistranslation/major), 존경을(mistr...      10.0      1.0   \n","4  Accuracy: Amina, Twitter(untranslated text/maj...      10.0     10.0   \n","\n","   style  total  \n","0    0.0   40.0  \n","1    0.0    7.0  \n","2   10.0   15.0  \n","3    0.0   11.0  \n","4    0.0   20.0  "],"text/html":["\n","  <div id=\"df-cd4ef8ac-9f79-4942-a684-94a7d5fc6da5\" class=\"colab-df-container\">\n","    <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>type</th>\n","      <th>corpus</th>\n","      <th>en_source</th>\n","      <th>ko_reference</th>\n","      <th>ko_target</th>\n","      <th>annotation</th>\n","      <th>accuracy</th>\n","      <th>fluency</th>\n","      <th>style</th>\n","      <th>total</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Training</td>\n","      <td>GlobalVoices</td>\n","      <td>To let Japanese buy FT will be the last thing ...</td>\n","      <td>중국은 닛케이가 파이낸셜타임스를 인수하는 걸 절대 바라지 않는다.</td>\n","      <td>중국은 일본 고객의 FT 구매를 최종 선호로 보고 싶지 않을 것입니다.</td>\n","      <td>Accuracy: 일본 고객의(mistranslation/major), 최종 선호로...</td>\n","      <td>35.0</td>\n","      <td>5.0</td>\n","      <td>0.0</td>\n","      <td>40.0</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Training</td>\n","      <td>GlobalVoices</td>\n","      <td>If our uncritical engagement with media is any...</td>\n","      <td>미디어와의 이런 안식없는 관계는 우리가 스스로와도 밀접하게 연관되어 있지 못하는 것...</td>\n","      <td>우리가 언론을 세심하게 살피지 않는 걸 보면 우리 일에는 관심조차 없는 것 같다.</td>\n","      <td>Accuracy: 우리가 언론을 세심하게 살피지 않는 걸 보면(mistranslat...</td>\n","      <td>7.0</td>\n","      <td>0.0</td>\n","      <td>0.0</td>\n","      <td>7.0</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Training</td>\n","      <td>GlobalVoices</td>\n","      <td>China: Rising prices and rooftop gardens · Glo...</td>\n","      <td>중국: 물가상승에 따른 새로운 현상- 옥상텃밭</td>\n","      <td>글로벌 보이스(Global Voices)는 중국의 치솟는 물가와 옥상 정원의 등장에...</td>\n","      <td>Accuracy: 등장에(addition/minor), (Global Voices)...</td>\n","      <td>3.0</td>\n","      <td>2.0</td>\n","      <td>10.0</td>\n","      <td>15.0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Training</td>\n","      <td>GlobalVoices</td>\n","      <td>And one year of a brave stance against great e...</td>\n","      <td>그리고 일 년 간 거대한 악에 대항하는 용기도 볼 수 있었다. 그러나 시리아인들은 ...</td>\n","      <td>1년 내내 강력한 세력에 맞서는 용기를 보여줬음에도 불구하고, 시리아 시민들은 자유...</td>\n","      <td>Accuracy: 세력에(mistranslation/major), 존경을(mistr...</td>\n","      <td>10.0</td>\n","      <td>1.0</td>\n","      <td>0.0</td>\n","      <td>11.0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Training</td>\n","      <td>GlobalVoices</td>\n","      <td>Amina's story deeply touched and outraged Moro...</td>\n","      <td>모로코 누리꾼들은 아미나의 이야기를 듣고 분노했고, 트위터 해쉬태그 #RIPAmin...</td>\n","      <td>Amina의 이야기는 Twitter에서 해시태그 #RIPAmina를 사용하여 소녀를...</td>\n","      <td>Accuracy: Amina, Twitter(untranslated text/maj...</td>\n","      <td>10.0</td>\n","      <td>10.0</td>\n","      <td>0.0</td>\n","      <td>20.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","    <div class=\"colab-df-buttons\">\n","\n","  <div class=\"colab-df-container\">\n","    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cd4ef8ac-9f79-4942-a684-94a7d5fc6da5')\"\n","            title=\"Convert this dataframe to an interactive table.\"\n","            style=\"display:none;\">\n","\n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n","    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n","  </svg>\n","    </button>\n","\n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    .colab-df-buttons div {\n","      margin-bottom: 4px;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","    <script>\n","      const buttonEl =\n","        document.querySelector('#df-cd4ef8ac-9f79-4942-a684-94a7d5fc6da5 button.colab-df-convert');\n","      buttonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","      async function convertToInteractive(key) {\n","        const element = document.querySelector('#df-cd4ef8ac-9f79-4942-a684-94a7d5fc6da5');\n","        const dataTable =\n","          await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                    [key], {});\n","        if (!dataTable) return;\n","\n","        const docLinkHtml = 'Like what you see? Visit the ' +\n","          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","          + ' to learn more about interactive tables.';\n","        element.innerHTML = '';\n","        dataTable['output_type'] = 'display_data';\n","        await google.colab.output.renderOutput(dataTable, element);\n","        const docLink = document.createElement('div');\n","        docLink.innerHTML = docLinkHtml;\n","        element.appendChild(docLink);\n","      }\n","    </script>\n","  </div>\n","\n","\n","<div id=\"df-cd6f575e-6889-4c33-b1cf-fb43249ef26b\">\n","  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cd6f575e-6889-4c33-b1cf-fb43249ef26b')\"\n","            title=\"Suggest charts.\"\n","            style=\"display:none;\">\n","\n","<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","     width=\"24px\">\n","    <g>\n","        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n","    </g>\n","</svg>\n","  </button>\n","\n","<style>\n","  .colab-df-quickchart {\n","      --bg-color: #E8F0FE;\n","      --fill-color: #1967D2;\n","      --hover-bg-color: #E2EBFA;\n","      --hover-fill-color: #174EA6;\n","      --disabled-fill-color: #AAA;\n","      --disabled-bg-color: #DDD;\n","  }\n","\n","  [theme=dark] .colab-df-quickchart {\n","      --bg-color: #3B4455;\n","      --fill-color: #D2E3FC;\n","      --hover-bg-color: #434B5C;\n","      --hover-fill-color: #FFFFFF;\n","      --disabled-bg-color: #3B4455;\n","      --disabled-fill-color: #666;\n","  }\n","\n","  .colab-df-quickchart {\n","    background-color: var(--bg-color);\n","    border: none;\n","    border-radius: 50%;\n","    cursor: pointer;\n","    display: none;\n","    fill: var(--fill-color);\n","    height: 32px;\n","    padding: 0;\n","    width: 32px;\n","  }\n","\n","  .colab-df-quickchart:hover {\n","    background-color: var(--hover-bg-color);\n","    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n","    fill: var(--button-hover-fill-color);\n","  }\n","\n","  .colab-df-quickchart-complete:disabled,\n","  .colab-df-quickchart-complete:disabled:hover {\n","    background-color: var(--disabled-bg-color);\n","    fill: var(--disabled-fill-color);\n","    box-shadow: none;\n","  }\n","\n","  .colab-df-spinner {\n","    border: 2px solid var(--fill-color);\n","    border-color: transparent;\n","    border-bottom-color: var(--fill-color);\n","    animation:\n","      spin 1s steps(1) infinite;\n","  }\n","\n","  @keyframes spin {\n","    0% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","      border-left-color: var(--fill-color);\n","    }\n","    20% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    30% {\n","      border-color: transparent;\n","      border-left-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","      border-right-color: var(--fill-color);\n","    }\n","    40% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-top-color: var(--fill-color);\n","    }\n","    60% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","    }\n","    80% {\n","      border-color: transparent;\n","      border-right-color: var(--fill-color);\n","      border-bottom-color: var(--fill-color);\n","    }\n","    90% {\n","      border-color: transparent;\n","      border-bottom-color: var(--fill-color);\n","    }\n","  }\n","</style>\n","\n","  <script>\n","    async function quickchart(key) {\n","      const quickchartButtonEl =\n","        document.querySelector('#' + key + ' button');\n","      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n","      quickchartButtonEl.classList.add('colab-df-spinner');\n","      try {\n","        const charts = await google.colab.kernel.invokeFunction(\n","            'suggestCharts', [key], {});\n","      } catch (error) {\n","        console.error('Error during call to suggestCharts:', error);\n","      }\n","      quickchartButtonEl.classList.remove('colab-df-spinner');\n","      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n","    }\n","    (() => {\n","      let quickchartButtonEl =\n","        document.querySelector('#df-cd6f575e-6889-4c33-b1cf-fb43249ef26b button');\n","      quickchartButtonEl.style.display =\n","        google.colab.kernel.accessAllowed ? 'block' : 'none';\n","    })();\n","  </script>\n","</div>\n","    </div>\n","  </div>\n"]},"metadata":{},"execution_count":4}],"source":["import pandas as pd\n","\n","df = pd.read_csv('drive/MyDrive/corpus/en_to_ko_mqm_evaluation.tsv', sep='\\t')\n","df.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rRCI6mThBlfM"},"outputs":[],"source":["train_data = df[:1000]\n","valid_data = df[1000:1100]\n","test_data = df[1100:]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1696005794245,"user":{"displayName":"Dojun Park","userId":"14496627616213037898"},"user_tz":-540},"id":"L2TtgQo4b-YC","outputId":"5d001183-75ae-4010-d409-1fa9e8bbd9eb"},"outputs":[{"output_type":"stream","name":"stdout","text":["1000\n","100\n","100\n"]}],"source":["print(len(train_data))\n","print(len(valid_data))\n","print(len(test_data))"]},{"cell_type":"markdown","metadata":{"id":"P0B77WsqCjbt"},"source":["# Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UrCS8NHniEU"},"outputs":[],"source":["import transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZuVDayULniB5"},"outputs":[],"source":["# RemBERT\n","\n","rembert_tokenizer = transformers.AutoTokenizer.from_pretrained(\"google/rembert\")\n","# rembert_model_mte = transformers.AutoModel.from_pretrained(\"google/rembert\")\n","rembert_model_qe = transformers.AutoModel.from_pretrained(\"google/rembert\")"]},{"cell_type":"markdown","metadata":{"id":"YjC1dk0eQZUb"},"source":["# Preprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1696005817098,"user":{"displayName":"Dojun Park","userId":"14496627616213037898"},"user_tz":-540},"id":"ftAxVJLUM2gp","outputId":"16a62e05-1c0b-4eaf-ce1d-02f526e2cb89"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["(1000, 1000, 100, 100, 100, 100)"]},"metadata":{},"execution_count":10}],"source":["from torch.utils.data import Dataset, DataLoader\n","import torch\n","import random\n","import numpy as np\n","\n","# set the random seed\n","def set_seed(seed_value=42):\n","    random.seed(seed_value)\n","    np.random.seed(seed_value)\n","    torch.manual_seed(seed_value)\n","    torch.cuda.manual_seed_all(seed_value)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","\n","set_seed(42)\n","\n","class MTEvaluationDataset(Dataset):\n","    def __init__(self, en_source, ko_reference, ko_target, accuracy, fluency, style, total, tokenizer, max_length=512):\n","        self.en_source = en_source\n","        self.ko_reference = ko_reference\n","        self.ko_target = ko_target\n","        self.accuracy = accuracy\n","        self.fluency = fluency\n","        self.style = style\n","        self.total = total\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.en_source)\n","\n","    def __getitem__(self, idx):\n","        # Concatenate the source, reference, and target texts with [SEP] tokens in between\n","        text = self.en_source[idx] + ' [SEP] ' + self.ko_reference[idx] + ' [SEP] ' + self.ko_target[idx]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            return_token_type_ids=True,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'accuracy': self.accuracy[idx],\n","            'fluency': self.fluency[idx],\n","            'style': self.style[idx],\n","            'total': self.total[idx]\n","        }\n","\n","class QEDataset(Dataset):\n","    def __init__(self, en_source, ko_target, accuracy, fluency, style, total, tokenizer, max_length=512):\n","        self.en_source = en_source\n","        self.ko_target = ko_target\n","        self.accuracy = accuracy\n","        self.fluency = fluency\n","        self.style = style\n","        self.total = total\n","        self.tokenizer = tokenizer\n","        self.max_length = max_length\n","\n","    def __len__(self):\n","        return len(self.en_source)\n","\n","    def __getitem__(self, idx):\n","        # Concatenate the source and target texts with [SEP] tokens in between\n","        text = self.en_source[idx] + ' [SEP] ' + self.ko_target[idx]\n","        encoding = self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            return_token_type_ids=True,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","\n","        return {\n","            'input_ids': encoding['input_ids'].flatten(),\n","            'attention_mask': encoding['attention_mask'].flatten(),\n","            'accuracy': self.accuracy[idx],\n","            'fluency': self.fluency[idx],\n","            'style': self.style[idx],\n","            'total': self.total[idx]\n","        }\n","\n","# Create datasets\n","mt_train_dataset = MTEvaluationDataset(\n","    train_data['en_source'].tolist(),\n","    train_data['ko_reference'].tolist(),\n","    train_data['ko_target'].tolist(),\n","    train_data['accuracy'].tolist(),\n","    train_data['fluency'].tolist(),\n","    train_data['style'].tolist(),\n","    train_data['total'].tolist(),\n","    rembert_tokenizer\n",")\n","\n","qe_train_dataset = QEDataset(\n","    train_data['en_source'].tolist(),\n","    train_data['ko_target'].tolist(),\n","    train_data['accuracy'].tolist(),\n","    train_data['fluency'].tolist(),\n","    train_data['style'].tolist(),\n","    train_data['total'].tolist(),\n","    rembert_tokenizer\n",")\n","\n","# For validation\n","mt_valid_dataset = MTEvaluationDataset(\n","    valid_data['en_source'].tolist(),\n","    valid_data['ko_reference'].tolist(),\n","    valid_data['ko_target'].tolist(),\n","    valid_data['accuracy'].tolist(),\n","    valid_data['fluency'].tolist(),\n","    valid_data['style'].tolist(),\n","    valid_data['total'].tolist(),\n","    rembert_tokenizer\n",")\n","\n","qe_valid_dataset = QEDataset(\n","    valid_data['en_source'].tolist(),\n","    valid_data['ko_target'].tolist(),\n","    valid_data['accuracy'].tolist(),\n","    valid_data['fluency'].tolist(),\n","    valid_data['style'].tolist(),\n","    valid_data['total'].tolist(),\n","    rembert_tokenizer\n",")\n","\n","# For test\n","mt_test_dataset = MTEvaluationDataset(\n","    test_data['en_source'].tolist(),\n","    test_data['ko_reference'].tolist(),\n","    test_data['ko_target'].tolist(),\n","    test_data['accuracy'].tolist(),\n","    test_data['fluency'].tolist(),\n","    test_data['style'].tolist(),\n","    test_data['total'].tolist(),\n","    rembert_tokenizer\n",")\n","\n","qe_test_dataset = QEDataset(\n","    test_data['en_source'].tolist(),\n","    test_data['ko_target'].tolist(),\n","    test_data['accuracy'].tolist(),\n","    test_data['fluency'].tolist(),\n","    test_data['style'].tolist(),\n","    test_data['total'].tolist(),\n","    rembert_tokenizer\n",")\n","\n","len(mt_train_dataset), len(qe_train_dataset), len(mt_valid_dataset), len(qe_valid_dataset), len(mt_test_dataset), len(qe_test_dataset)"]},{"cell_type":"markdown","metadata":{"id":"kXw82ZF2ROcs"},"source":["## Modeling"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IR5TzPWxM2cP"},"outputs":[],"source":["import torch.nn as nn\n","\n","class MTEvaluationModel(nn.Module):\n","    def __init__(self, base_model):\n","        super(MTEvaluationModel, self).__init__()\n","        self.base_model = base_model\n","        self.regression_head = nn.Linear(base_model.config.hidden_size, 3)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        return self.regression_head(pooled_output)\n","\n","class QEModel(nn.Module):\n","    def __init__(self, base_model):\n","        super(QEModel, self).__init__()\n","        self.base_model = base_model\n","        self.regression_head = nn.Linear(base_model.config.hidden_size, 3)\n","\n","    def forward(self, input_ids, attention_mask):\n","        outputs = self.base_model(input_ids, attention_mask=attention_mask)\n","        pooled_output = outputs[1]\n","        return self.regression_head(pooled_output)\n","\n","# Instantiate models\n","# mt_evaluation_model = MTEvaluationModel(rembert_model_mte)\n","qe_model = QEModel(rembert_model_qe)"]},{"cell_type":"markdown","metadata":{"id":"y61c24QdRgH4"},"source":["\n","## Run training"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7R_g-dCrec-K","executionInfo":{"status":"ok","timestamp":1696018017063,"user_tz":-540,"elapsed":12170634,"user":{"displayName":"Dojun Park","userId":"14496627616213037898"}},"outputId":"ac83f7ec-b21b-4beb-cbc2-45107bbea1e5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/100\n","QE Model - Train Loss: 85.3821, Validation Loss: 62.8862\n","============================================================\n","Epoch 2/100\n","QE Model - Train Loss: 69.8551, Validation Loss: 59.8068\n","============================================================\n","Epoch 3/100\n","QE Model - Train Loss: 66.6030, Validation Loss: 58.4071\n","============================================================\n","Epoch 4/100\n","QE Model - Train Loss: 64.5051, Validation Loss: 56.9902\n","============================================================\n","Epoch 5/100\n","QE Model - Train Loss: 60.1128, Validation Loss: 55.5602\n","============================================================\n","Epoch 6/100\n","QE Model - Train Loss: 55.9617, Validation Loss: 54.5779\n","============================================================\n","Epoch 7/100\n","QE Model - Train Loss: 51.4000, Validation Loss: 55.0315\n","============================================================\n","Epoch 8/100\n","QE Model - Train Loss: 47.6682, Validation Loss: 54.5992\n","============================================================\n","Epoch 9/100\n","QE Model - Train Loss: 46.2100, Validation Loss: 51.9353\n","============================================================\n","Epoch 10/100\n","QE Model - Train Loss: 44.7531, Validation Loss: 61.6753\n","============================================================\n","Epoch 11/100\n","QE Model - Train Loss: 42.4719, Validation Loss: 54.0371\n","============================================================\n","Epoch 12/100\n","QE Model - Train Loss: 39.8605, Validation Loss: 52.7883\n","============================================================\n","Epoch 13/100\n","QE Model - Train Loss: 38.3008, Validation Loss: 51.6513\n","============================================================\n","Epoch 14/100\n","QE Model - Train Loss: 36.3832, Validation Loss: 49.7097\n","============================================================\n","Epoch 15/100\n","QE Model - Train Loss: 34.5612, Validation Loss: 49.4773\n","============================================================\n","Epoch 16/100\n","QE Model - Train Loss: 32.8783, Validation Loss: 47.6485\n","============================================================\n","Epoch 17/100\n","QE Model - Train Loss: 31.2449, Validation Loss: 48.3983\n","============================================================\n","Epoch 18/100\n","QE Model - Train Loss: 29.7861, Validation Loss: 48.9370\n","============================================================\n","Epoch 19/100\n","QE Model - Train Loss: 28.8887, Validation Loss: 48.1329\n","============================================================\n","Epoch 20/100\n","QE Model - Train Loss: 27.6092, Validation Loss: 48.5331\n","============================================================\n","Epoch 21/100\n","QE Model - Train Loss: 26.5821, Validation Loss: 47.5975\n","============================================================\n","Epoch 22/100\n","QE Model - Train Loss: 25.6134, Validation Loss: 47.3851\n","============================================================\n","Epoch 23/100\n","QE Model - Train Loss: 24.8536, Validation Loss: 48.4146\n","============================================================\n","Epoch 24/100\n","QE Model - Train Loss: 24.2195, Validation Loss: 48.6959\n","============================================================\n","Epoch 25/100\n","QE Model - Train Loss: 23.5631, Validation Loss: 47.7454\n","============================================================\n","Epoch 26/100\n","QE Model - Train Loss: 22.9957, Validation Loss: 48.1145\n","============================================================\n","Epoch 27/100\n","QE Model - Train Loss: 22.5283, Validation Loss: 46.8004\n","============================================================\n","Epoch 28/100\n","QE Model - Train Loss: 22.0411, Validation Loss: 46.3142\n","============================================================\n","Epoch 29/100\n","QE Model - Train Loss: 21.5012, Validation Loss: 47.0367\n","============================================================\n","Epoch 30/100\n","QE Model - Train Loss: 21.0061, Validation Loss: 46.5154\n","============================================================\n","Epoch 31/100\n","QE Model - Train Loss: 20.6024, Validation Loss: 46.1770\n","============================================================\n","Epoch 32/100\n","QE Model - Train Loss: 20.1588, Validation Loss: 46.0251\n","============================================================\n","Epoch 33/100\n","QE Model - Train Loss: 19.5959, Validation Loss: 46.5905\n","============================================================\n","Epoch 34/100\n","QE Model - Train Loss: 19.1412, Validation Loss: 45.7103\n","============================================================\n","Epoch 35/100\n","QE Model - Train Loss: 18.7505, Validation Loss: 46.9422\n","============================================================\n","Epoch 36/100\n","QE Model - Train Loss: 18.3801, Validation Loss: 45.6006\n","============================================================\n","Epoch 37/100\n","QE Model - Train Loss: 18.0295, Validation Loss: 45.0184\n","============================================================\n","Epoch 38/100\n","QE Model - Train Loss: 17.7151, Validation Loss: 45.7702\n","============================================================\n","Epoch 39/100\n","QE Model - Train Loss: 17.3288, Validation Loss: 45.4849\n","============================================================\n","Epoch 40/100\n","QE Model - Train Loss: 17.1038, Validation Loss: 46.7474\n","============================================================\n","Epoch 41/100\n","QE Model - Train Loss: 16.9674, Validation Loss: 45.6363\n","============================================================\n","Epoch 42/100\n","QE Model - Train Loss: 16.6353, Validation Loss: 46.3453\n","============================================================\n","Epoch 43/100\n","QE Model - Train Loss: 16.0929, Validation Loss: 44.3645\n","============================================================\n","Epoch 44/100\n","QE Model - Train Loss: 15.6036, Validation Loss: 45.2253\n","============================================================\n","Epoch 45/100\n","QE Model - Train Loss: 15.1770, Validation Loss: 44.3848\n","============================================================\n","Epoch 46/100\n","QE Model - Train Loss: 14.8439, Validation Loss: 45.1457\n","============================================================\n","Epoch 47/100\n","QE Model - Train Loss: 14.5004, Validation Loss: 45.3730\n","============================================================\n","Epoch 48/100\n","QE Model - Train Loss: 14.2067, Validation Loss: 44.8222\n","============================================================\n","Epoch 49/100\n","QE Model - Train Loss: 13.9838, Validation Loss: 45.4433\n","============================================================\n","Epoch 50/100\n","QE Model - Train Loss: 13.7925, Validation Loss: 44.8111\n","============================================================\n","Epoch 51/100\n","QE Model - Train Loss: 13.5687, Validation Loss: 46.5483\n","============================================================\n","Epoch 52/100\n","QE Model - Train Loss: 13.3739, Validation Loss: 46.0122\n","============================================================\n","Epoch 53/100\n","QE Model - Train Loss: 13.1037, Validation Loss: 45.8628\n","============================================================\n","Epoch 54/100\n","QE Model - Train Loss: 12.8162, Validation Loss: 45.8749\n","============================================================\n","Epoch 55/100\n","QE Model - Train Loss: 12.5409, Validation Loss: 45.4399\n","============================================================\n","Epoch 56/100\n","QE Model - Train Loss: 12.3420, Validation Loss: 45.1120\n","============================================================\n","Epoch 57/100\n","QE Model - Train Loss: 12.0923, Validation Loss: 45.7633\n","============================================================\n","Epoch 58/100\n","QE Model - Train Loss: 11.8618, Validation Loss: 45.0751\n","============================================================\n","Epoch 59/100\n","QE Model - Train Loss: 11.7272, Validation Loss: 45.3547\n","============================================================\n","Epoch 60/100\n","QE Model - Train Loss: 11.5600, Validation Loss: 45.4363\n","============================================================\n","Epoch 61/100\n","QE Model - Train Loss: 11.4094, Validation Loss: 46.0701\n","============================================================\n","Epoch 62/100\n","QE Model - Train Loss: 11.1491, Validation Loss: 45.8535\n","============================================================\n","Epoch 63/100\n","QE Model - Train Loss: 10.8572, Validation Loss: 45.0002\n","============================================================\n","Epoch 64/100\n","QE Model - Train Loss: 10.6486, Validation Loss: 44.8549\n","============================================================\n","Epoch 65/100\n","QE Model - Train Loss: 10.4578, Validation Loss: 44.6473\n","============================================================\n","Epoch 66/100\n","QE Model - Train Loss: 10.4048, Validation Loss: 45.5540\n","============================================================\n","Epoch 67/100\n","QE Model - Train Loss: 10.2272, Validation Loss: 46.3575\n","============================================================\n","Epoch 68/100\n","QE Model - Train Loss: 10.0197, Validation Loss: 45.9087\n","============================================================\n","Epoch 69/100\n","QE Model - Train Loss: 9.8375, Validation Loss: 46.9923\n","============================================================\n","Epoch 70/100\n","QE Model - Train Loss: 9.5627, Validation Loss: 45.5166\n","============================================================\n","Epoch 71/100\n","QE Model - Train Loss: 9.3076, Validation Loss: 46.6644\n","============================================================\n","Epoch 72/100\n","QE Model - Train Loss: 9.1077, Validation Loss: 46.5782\n","============================================================\n","Epoch 73/100\n","QE Model - Train Loss: 8.9319, Validation Loss: 45.8423\n","============================================================\n","Epoch 74/100\n","QE Model - Train Loss: 8.7613, Validation Loss: 46.1686\n","============================================================\n","Epoch 75/100\n","QE Model - Train Loss: 8.6467, Validation Loss: 46.1778\n","============================================================\n","Epoch 76/100\n","QE Model - Train Loss: 8.4981, Validation Loss: 47.2261\n","============================================================\n","Epoch 77/100\n","QE Model - Train Loss: 8.3519, Validation Loss: 46.0449\n","============================================================\n","Epoch 78/100\n","QE Model - Train Loss: 8.2334, Validation Loss: 45.9309\n","============================================================\n","Epoch 79/100\n","QE Model - Train Loss: 8.1132, Validation Loss: 47.1060\n","============================================================\n","Epoch 80/100\n","QE Model - Train Loss: 8.0142, Validation Loss: 46.9895\n","============================================================\n","Epoch 81/100\n","QE Model - Train Loss: 7.9201, Validation Loss: 46.3707\n","============================================================\n","Epoch 82/100\n","QE Model - Train Loss: 7.8192, Validation Loss: 47.7121\n","============================================================\n","Epoch 83/100\n","QE Model - Train Loss: 7.6623, Validation Loss: 45.7059\n","============================================================\n","Epoch 84/100\n","QE Model - Train Loss: 7.5690, Validation Loss: 47.3713\n","============================================================\n","Epoch 85/100\n","QE Model - Train Loss: 7.4693, Validation Loss: 47.2319\n","============================================================\n","Epoch 86/100\n","QE Model - Train Loss: 7.2424, Validation Loss: 45.2571\n","============================================================\n","Epoch 87/100\n","QE Model - Train Loss: 7.0373, Validation Loss: 46.6046\n","============================================================\n","Epoch 88/100\n","QE Model - Train Loss: 6.8457, Validation Loss: 47.0727\n","============================================================\n","Epoch 89/100\n","QE Model - Train Loss: 6.6997, Validation Loss: 45.5789\n","============================================================\n","Epoch 90/100\n","QE Model - Train Loss: 6.5890, Validation Loss: 45.9770\n","============================================================\n","Epoch 91/100\n","QE Model - Train Loss: 6.4699, Validation Loss: 45.7367\n","============================================================\n","Epoch 92/100\n","QE Model - Train Loss: 6.3651, Validation Loss: 46.1172\n","============================================================\n","Epoch 93/100\n","QE Model - Train Loss: 6.2506, Validation Loss: 46.3429\n","============================================================\n","Epoch 94/100\n","QE Model - Train Loss: 6.1613, Validation Loss: 47.3215\n","============================================================\n","Epoch 95/100\n","QE Model - Train Loss: 6.0433, Validation Loss: 45.8881\n","============================================================\n","Epoch 96/100\n","QE Model - Train Loss: 5.9517, Validation Loss: 45.8346\n","============================================================\n","Epoch 97/100\n","QE Model - Train Loss: 5.9081, Validation Loss: 46.5026\n","============================================================\n","Epoch 98/100\n","QE Model - Train Loss: 5.9034, Validation Loss: 45.4126\n","============================================================\n","Epoch 99/100\n","QE Model - Train Loss: 5.8160, Validation Loss: 48.4024\n","============================================================\n","Epoch 100/100\n","QE Model - Train Loss: 5.7313, Validation Loss: 48.4237\n","============================================================\n"]}],"source":["from torch.optim import AdamW\n","from scipy.stats import spearmanr\n","from torch.utils.data import DataLoader\n","import torch\n","import numpy as np\n","import random\n","from scipy.stats import kendalltau\n","import sys\n","\n","# Hyperparameters\n","EPOCHS = 100\n","BATCH_SIZE = 8\n","LEARNING_RATE = 2e-6\n","DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","# Move models to device\n","# mt_evaluation_model = mt_evaluation_model.float().to(DEVICE)\n","qe_model = qe_model.float().to(DEVICE)\n","\n","# Optimizers\n","# mt_optimizer = AdamW(mt_evaluation_model.parameters(), lr=LEARNING_RATE)\n","qe_optimizer = AdamW(qe_model.parameters(), lr=LEARNING_RATE)\n","\n","# Loss function\n","loss_fn = nn.MSELoss()\n","\n","# DataLoaders\n","# mt_train_dataloader = DataLoader(mt_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","# mt_valid_dataloader = DataLoader(mt_valid_dataset, batch_size=BATCH_SIZE)\n","# mt_test_dataloader = DataLoader(mt_test_dataset, batch_size=BATCH_SIZE)\n","qe_train_dataloader = DataLoader(qe_train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","qe_valid_dataloader = DataLoader(qe_valid_dataset, batch_size=BATCH_SIZE)\n","qe_test_dataloader = DataLoader(qe_test_dataset, batch_size=BATCH_SIZE)\n","\n","\n","# path to save the model weights\n","# MODEL_SAVE_PATH = 'drive/MyDrive/model/ex1/rembert/mte/'\n","MODEL_SAVE_PATH = 'drive/MyDrive/model/ex1/rembert/qe/'\n","\n","# Training loop\n","def train_model(model, optimizer, dataloader):\n","    model.train()\n","    total_loss = 0\n","    for data in dataloader:\n","        input_ids = data['input_ids'].to(DEVICE)\n","        attention_mask = data['attention_mask'].to(DEVICE)\n","        targets = torch.stack([data['accuracy'], data['fluency'], data['style']], dim=1).to(DEVICE).float()\n","\n","        optimizer.zero_grad()\n","        outputs = model(input_ids, attention_mask)\n","        loss = loss_fn(outputs, targets)\n","        loss.backward()\n","        optimizer.step()\n","\n","        total_loss += loss.item()\n","    return total_loss / len(dataloader)\n","\n","# Validation loop\n","def validate_model(model, dataloader):\n","    model.eval()\n","    total_val_loss = 0\n","\n","    all_preds = []\n","    all_targets = []\n","\n","    with torch.no_grad():\n","        for data in dataloader:\n","            input_ids = data['input_ids'].to(DEVICE)\n","            attention_mask = data['attention_mask'].to(DEVICE)\n","            targets = torch.stack([data['accuracy'], data['fluency'], data['style']], dim=1).to(DEVICE)\n","\n","            outputs = model(input_ids, attention_mask)\n","            all_preds.append(outputs.cpu())\n","            all_targets.append(targets.cpu())\n","\n","            val_loss = loss_fn(outputs, targets)\n","            total_val_loss += val_loss.item()\n","\n","    avg_val_loss = total_val_loss / len(dataloader)\n","\n","    all_preds = torch.cat(all_preds, dim=0).numpy()\n","    all_targets = torch.cat(all_targets, dim=0).numpy()\n","\n","    return avg_val_loss\n","\n","def test_model(model, dataloader):\n","    model.eval()\n","    all_preds = []\n","    all_targets = []\n","    total_scores = []\n","    with torch.no_grad():\n","        for data in dataloader:\n","            input_ids = data['input_ids'].to(DEVICE)\n","            attention_mask = data['attention_mask'].to(DEVICE)\n","            targets = torch.stack([data['accuracy'], data['fluency'], data['style']], dim=1).to(DEVICE)\n","\n","            outputs = model(input_ids, attention_mask)\n","            all_preds.append(outputs.cpu())\n","            all_targets.append(targets.cpu())\n","            total_scores.extend(data['total'].tolist())\n","\n","    all_preds = torch.cat(all_preds, dim=0).numpy()\n","    all_targets = torch.cat(all_targets, dim=0).numpy()\n","\n","    kendall_accuracy = kendalltau(all_preds[:, 0], all_targets[:, 0])[0]\n","    kendall_fluency = kendalltau(all_preds[:, 1], all_targets[:, 1])[0]\n","    kendall_style = kendalltau(all_preds[:, 2], all_targets[:, 2])[0]\n","\n","    # Compute accumulated scores from the predictions\n","    accumulated_preds = all_preds.sum(axis=1)\n","    kendall_total = kendalltau(accumulated_preds, total_scores)[0]\n","\n","    return kendall_accuracy, kendall_fluency, kendall_style, kendall_total\n","\n","# Training and validation execution\n","g_val_loss = sys.maxsize\n","for epoch in range(EPOCHS):\n","    # mt_train_loss = train_model(mt_evaluation_model, mt_optimizer, mt_train_dataloader)\n","    # mt_val_loss = validate_model(mt_evaluation_model, mt_valid_dataloader)\n","    qe_train_loss = train_model(qe_model, qe_optimizer, qe_train_dataloader)\n","    qe_val_loss = validate_model(qe_model, qe_valid_dataloader)\n","\n","    # Save the model at each epoch\n","    if g_val_loss >= qe_val_loss:\n","        # torch.save(mt_evaluation_model.state_dict(), f\"{MODEL_SAVE_PATH}rembert_mte_1000_training1_epoch_{epoch+1}.pt\")\n","        torch.save(qe_model.state_dict(), f\"{MODEL_SAVE_PATH}rembert_qe_1000_training1_epoch_{epoch+1}.pt\")\n","        g_val_loss = qe_val_loss\n","\n","    print(f\"Epoch {epoch+1}/{EPOCHS}\")\n","    # print(f\"MT Evaluation Model - Train Loss: {mt_train_loss:.4f}, Validation Loss: {mt_val_loss:.4f}\")\n","    print(f\"QE Model - Train Loss: {qe_train_loss:.4f}, Validation Loss: {qe_val_loss:.4f}\")\n","    print(\"=\"*60)"]},{"cell_type":"markdown","metadata":{"id":"K5dwp4HUisY4"},"source":["## Test the model"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":36170,"status":"ok","timestamp":1693970020086,"user":{"displayName":"chat gpt","userId":"17581220294899742902"},"user_tz":-540},"id":"9dF9DHxWGsbh","outputId":"8ea84a9c-d192-4046-f3bf-76219f2034f6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 97 >>> Test Kendall Accuracy: 0.3743, Fluency: 0.3710, Style: 0.2400, Total: 0.3630, Average: 0.3284\n"]}],"source":["i = 97\n","\n","# path to the saved model weights\n","# SAVED_MODEL_PATH = f'drive/MyDrive/model/ex1/rembert/mte/rembert_mte_1000_training1_epoch_{i}.pt'\n","SAVED_MODEL_PATH = f'drive/MyDrive/model/ex1/rembert/qe/rembert_qe_1000_training1_epoch_{i}.pt'\n","\n","# load the model\n","# loaded_model = MTEvaluationModel(rembert_model_mte)\n","loaded_model = QEModel(rembert_model_qe)\n","\n","# Load the saved weights into the model\n","loaded_model.load_state_dict(torch.load(SAVED_MODEL_PATH))\n","\n","# Move the model to the device\n","loaded_model = loaded_model.to(DEVICE)\n","\n","# Set the model to evaluation mode\n","loaded_model.eval()\n","\n","# calculate kendall's tau correlations on the test set\n","# mt_kendall_accuracy, mt_kendall_fluency, mt_kendall_style, mt_kendall_total = test_model(loaded_model, mt_test_dataloader)\n","# mt_kendall_average = (mt_kendall_accuracy + mt_kendall_fluency + mt_kendall_style) / 3\n","# print(f\"Epoch {i} >>> Test Kendall Accuracy: {mt_kendall_accuracy:.4f}, Fluency: {mt_kendall_fluency:.4f}, Style: {mt_kendall_style:.4f}, Total: {mt_kendall_total:.4f}, Average: {mt_kendall_average:.4f}\")\n","\n","qe_kendall_accuracy, qe_kendall_fluency, qe_kendall_style, qe_kendall_total = test_model(loaded_model, qe_test_dataloader)\n","qe_kendall_average = (qe_kendall_accuracy + qe_kendall_fluency + qe_kendall_style) / 3\n","print(f\"Epoch {i} >>> Test Kendall Accuracy: {qe_kendall_accuracy:.4f}, Fluency: {qe_kendall_fluency:.4f}, Style: {qe_kendall_style:.4f}, Total: {qe_kendall_total:.4f}, Average: {qe_kendall_average:.4f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wjPXS9vAYWOB"},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULUoT8KFYWLh"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"A100"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}